{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS340-H Final Capstone Project\n",
    "Jennifer Ruffin\n",
    "\n",
    "**_Research Question: How does station-level demand (in terms of trip origins  and destinations) vary across different months and times of day, and what are the resulting peak usage periods for the most popular stations?_**\n",
    "\n",
    "In this notebook, I will employ the methods Logistic Regression, Gradient Boosting, and Random Forests to predict which stations are more likely to be used on a given day. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bikeData = pd.read_csv('/Users/jenniferruffin/Desktop/finalcapstone.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                     int64\n",
      "start_station_name            object\n",
      "end_station_name              object\n",
      "started_date                  object\n",
      "started_time          datetime64[ns]\n",
      "ended_date                    object\n",
      "ended_time            datetime64[ns]\n",
      "trip_duration                float64\n",
      "startmonth                    object\n",
      "endmonth                      object\n",
      "startTOD                      object\n",
      "endTOD                        object\n",
      "start_hour                     int64\n",
      "end_hour                       int64\n",
      "start_day_of_week             object\n",
      "end_day_of_week               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "bikeData['started_time'] = pd.to_datetime(bikeData['started_time'], errors='coerce')\n",
    "bikeData['ended_time'] = pd.to_datetime(bikeData['ended_time'], errors='coerce')\n",
    "\n",
    "print(bikeData.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar to what was done in my R code, below I am finding the total daily counts per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Activity per Station:\n",
      "           date                                            station  \\\n",
      "332  2025-04-07                       MIT at Mass Ave / Amherst St   \n",
      "131  2025-04-07              Central Square at Mass Ave / Essex St   \n",
      "267  2025-04-07                Harvard Square at Mass Ave/ Dunster   \n",
      "139  2025-04-07        Charles Circle - Charles St at Cambridge St   \n",
      "329  2025-04-07                    MIT Pacific St at Purrington St   \n",
      "39   2025-04-07                                 Ames St at Main St   \n",
      "152  2025-04-07  Christian Science Plaza - Massachusetts Ave at...   \n",
      "349  2025-04-07                          Mass Ave/Lafayette Square   \n",
      "89   2025-04-07                        Boylston St at Fairfield St   \n",
      "331  2025-04-07                                      MIT Vassar St   \n",
      "\n",
      "     start_count  end_count  total_activity  \n",
      "332      57004.0      56010        113014.0  \n",
      "131      47530.0      47527         95057.0  \n",
      "267      40938.0      42356         83294.0  \n",
      "139      34146.0      34087         68233.0  \n",
      "329      33278.0      32163         65441.0  \n",
      "39       28431.0      31530         59961.0  \n",
      "152      28714.0      28819         57533.0  \n",
      "349      28536.0      28891         57427.0  \n",
      "89       27683.0      27889         55572.0  \n",
      "331      28812.0      26001         54813.0  \n"
     ]
    }
   ],
   "source": [
    "# daily start counts per station\n",
    "daily_starts = bikeData.groupby([bikeData['started_time'].dt.date, 'start_station_name']).size().reset_index(name='start_count')\n",
    "daily_starts = daily_starts.rename(columns={'started_time': 'date', 'start_station_name': 'station'})\n",
    "\n",
    "# daily end counts per station\n",
    "daily_ends = bikeData.groupby([bikeData['ended_time'].dt.date, 'end_station_name']).size().reset_index(name='end_count')\n",
    "daily_ends = daily_ends.rename(columns={'ended_time': 'date', 'end_station_name': 'station'})\n",
    "\n",
    "# merge the two counts\n",
    "total_activity = pd.merge(daily_starts, daily_ends, on=['date', 'station'], how='outer').fillna(0)\n",
    "\n",
    "# get total daily counts\n",
    "total_activity['total_activity'] = total_activity['start_count'] + total_activity['end_count']\n",
    "\n",
    "print(\"Daily Activity per Station:\")\n",
    "print(total_activity.sort_values(by='total_activity', ascending=False).head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'total_activity' counts will now be used with the function defineHighUsage(). As a reminder, the point of this notebook is to predict which stations are more likely to be used\n",
    "on a given day. For a station to be a high usage station, I am setting the percentile threshold to 0.8 which is the top 20 percentile. I will use these specific sations for \n",
    "the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineHighUsage(daily_counts, percentile=0.8):\n",
    "    def get_threshold(group):\n",
    "        threshold = group['total_activity'].quantile(percentile)\n",
    "        return threshold\n",
    "\n",
    "    thresholds = daily_counts.groupby('date').apply(get_threshold).reset_index(name='threshold')\n",
    "    daily_counts_merged = pd.merge(daily_counts, thresholds, on='date')\n",
    "    daily_counts_merged['high_usage'] = (daily_counts_merged['total_activity'] >= daily_counts_merged['threshold']).astype(int)\n",
    "    return daily_counts_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling our function to see the stations in descending order, so highest total activity to lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date                                            station  \\\n",
      "332  2025-04-07                       MIT at Mass Ave / Amherst St   \n",
      "131  2025-04-07              Central Square at Mass Ave / Essex St   \n",
      "267  2025-04-07                Harvard Square at Mass Ave/ Dunster   \n",
      "139  2025-04-07        Charles Circle - Charles St at Cambridge St   \n",
      "329  2025-04-07                    MIT Pacific St at Purrington St   \n",
      "39   2025-04-07                                 Ames St at Main St   \n",
      "152  2025-04-07  Christian Science Plaza - Massachusetts Ave at...   \n",
      "349  2025-04-07                          Mass Ave/Lafayette Square   \n",
      "89   2025-04-07                        Boylston St at Fairfield St   \n",
      "331  2025-04-07                                      MIT Vassar St   \n",
      "\n",
      "     start_count  end_count  total_activity  threshold  high_usage  \n",
      "332      57004.0      56010        113014.0    20210.0           1  \n",
      "131      47530.0      47527         95057.0    20210.0           1  \n",
      "267      40938.0      42356         83294.0    20210.0           1  \n",
      "139      34146.0      34087         68233.0    20210.0           1  \n",
      "329      33278.0      32163         65441.0    20210.0           1  \n",
      "39       28431.0      31530         59961.0    20210.0           1  \n",
      "152      28714.0      28819         57533.0    20210.0           1  \n",
      "349      28536.0      28891         57427.0    20210.0           1  \n",
      "89       27683.0      27889         55572.0    20210.0           1  \n",
      "331      28812.0      26001         54813.0    20210.0           1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/z3wpn8pd38gf91rd2q55fgp00000gn/T/ipykernel_50837/2018631914.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  thresholds = daily_counts.groupby('date').apply(get_threshold).reset_index(name='threshold')\n"
     ]
    }
   ],
   "source": [
    "total_activity_with_usage = defineHighUsage(total_activity, percentile=0.8)\n",
    "\n",
    "print(total_activity_with_usage.sort_values(by='total_activity', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_I encountered a problem: Now, I have two dataframes with information that needs to be used altogether._**\n",
    "\n",
    "So, I merged my original dataset (bikeData) with my dataframe with my high usage information (total_activity_with_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in high_usage after merge: 962\n",
      "   Unnamed: 0  start_station_name                         end_station_name  \\\n",
      "0           1  Ames St at Main St    Central Square at Mass Ave / Essex St   \n",
      "1           2  Ames St at Main St    Central Square at Mass Ave / Essex St   \n",
      "2           3  One Memorial Drive  Kennedy-Longfellow School 158 Spring St   \n",
      "3           4  Ames St at Main St                      Brookline Town Hall   \n",
      "4           5  Mass Ave T Station                         Chinatown T Stop   \n",
      "\n",
      "  started_date        started_time  ended_date          ended_time  \\\n",
      "0   2024-01-31 2025-04-07 12:16:49  2024-01-31 2025-04-07 12:21:02   \n",
      "1   2024-01-12 2025-04-07 08:14:16  2024-01-12 2025-04-07 08:19:48   \n",
      "2   2024-01-29 2025-04-07 15:00:05  2024-01-29 2025-04-07 15:05:47   \n",
      "3   2024-01-09 2025-04-07 16:33:40  2024-01-09 2025-04-07 17:00:41   \n",
      "4   2024-01-23 2025-04-07 10:19:21  2024-01-23 2025-04-07 10:31:39   \n",
      "\n",
      "   trip_duration startmonth endmonth  ... start_day_of_week end_day_of_week  \\\n",
      "0       4.216667    January  January  ...         Wednesday       Wednesday   \n",
      "1       5.533333    January  January  ...            Friday          Friday   \n",
      "2       5.700000    January  January  ...            Monday          Monday   \n",
      "3      27.016667    January  January  ...           Tuesday         Tuesday   \n",
      "4      12.300000    January  January  ...           Tuesday         Tuesday   \n",
      "\n",
      "   start_date        date             station start_count end_count  \\\n",
      "0  2025-04-07  2025-04-07  Ames St at Main St     28431.0   31530.0   \n",
      "1  2025-04-07  2025-04-07  Ames St at Main St     28431.0   31530.0   \n",
      "2  2025-04-07  2025-04-07  One Memorial Drive      5715.0    5516.0   \n",
      "3  2025-04-07  2025-04-07  Ames St at Main St     28431.0   31530.0   \n",
      "4  2025-04-07  2025-04-07  Mass Ave T Station     10266.0   10410.0   \n",
      "\n",
      "  total_activity threshold  high_usage  \n",
      "0        59961.0   20210.0         1.0  \n",
      "1        59961.0   20210.0         1.0  \n",
      "2        11231.0   20210.0         0.0  \n",
      "3        59961.0   20210.0         1.0  \n",
      "4        20676.0   20210.0         1.0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "bikeData['start_date'] = bikeData['started_time'].dt.date\n",
    "bikeData_merged = pd.merge(bikeData, total_activity_with_usage,\n",
    "                           left_on=['start_date', 'start_station_name'],\n",
    "                           right_on=['date', 'station'],\n",
    "                           how='left')\n",
    "\n",
    "print(\"NaNs in high_usage after merge:\", bikeData_merged['high_usage'].isnull().sum())\n",
    "print(bikeData_merged.head())\n",
    "\n",
    "# Replace the original bikeData with the merged one\n",
    "bikeData = bikeData_merged.drop(columns=['start_date', 'date', 'station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                     int64\n",
      "start_station_name            object\n",
      "end_station_name              object\n",
      "started_date                  object\n",
      "started_time          datetime64[ns]\n",
      "ended_date                    object\n",
      "ended_time            datetime64[ns]\n",
      "trip_duration                float64\n",
      "startmonth                    object\n",
      "endmonth                      object\n",
      "startTOD                      object\n",
      "endTOD                        object\n",
      "start_hour                     int64\n",
      "end_hour                       int64\n",
      "start_day_of_week             object\n",
      "end_day_of_week               object\n",
      "start_count                  float64\n",
      "end_count                    float64\n",
      "total_activity               float64\n",
      "threshold                    float64\n",
      "high_usage                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(bikeData.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_We've now prepped our dataset and established high usage. So, now let's use the models_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding to handle our categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/z3wpn8pd38gf91rd2q55fgp00000gn/T/ipykernel_50837/704493789.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bikeData[col + '_encoded'] = encoders[col].fit_transform(bikeData[col])\n",
      "/var/folders/59/z3wpn8pd38gf91rd2q55fgp00000gn/T/ipykernel_50837/704493789.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bikeData[col + '_encoded'] = encoders[col].fit_transform(bikeData[col])\n",
      "/var/folders/59/z3wpn8pd38gf91rd2q55fgp00000gn/T/ipykernel_50837/704493789.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bikeData[col + '_encoded'] = encoders[col].fit_transform(bikeData[col])\n",
      "/var/folders/59/z3wpn8pd38gf91rd2q55fgp00000gn/T/ipykernel_50837/704493789.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bikeData[col + '_encoded'] = encoders[col].fit_transform(bikeData[col])\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['start_day_of_week', 'endTOD', 'startTOD', 'startmonth']\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    encoders[col] = LabelEncoder()\n",
    "    bikeData[col + '_encoded'] = encoders[col].fit_transform(bikeData[col])\n",
    "\n",
    "numerical_cols = [] # Add numerical predictors if you have them (e.g., lagged usage)\n",
    "scaler = None\n",
    "if numerical_cols:\n",
    "    scaler = StandardScaler()\n",
    "    bikeData[numerical_cols] = scaler.fit_transform(bikeData[numerical_cols])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "feature_cols = [col + '_encoded' for col in categorical_cols] + numerical_cols\n",
    "X = bikeData[feature_cols]\n",
    "y = bikeData['high_usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          Unnamed: 0              start_station_name  \\\n",
      "0                 1              Ames St at Main St   \n",
      "1                 2              Ames St at Main St   \n",
      "2                 3              One Memorial Drive   \n",
      "3                 4              Ames St at Main St   \n",
      "4                 5              Mass Ave T Station   \n",
      "...             ...                             ...   \n",
      "3179249     3179250      Washington St at Temple Pl   \n",
      "3179250     3179251  Boston City Hall - 28 State St   \n",
      "3179251     3179252  Boston City Hall - 28 State St   \n",
      "3179252     3179253  Boston City Hall - 28 State St   \n",
      "3179253     3179254           EF - North Point Park   \n",
      "\n",
      "                                     end_station_name started_date  \\\n",
      "0               Central Square at Mass Ave / Essex St   2024-01-31   \n",
      "1               Central Square at Mass Ave / Essex St   2024-01-12   \n",
      "2             Kennedy-Longfellow School 158 Spring St   2024-01-29   \n",
      "3                                 Brookline Town Hall   2024-01-09   \n",
      "4                                    Chinatown T Stop   2024-01-23   \n",
      "...                                               ...          ...   \n",
      "3179249  Government Center - Cambridge St at Court St   2025-01-09   \n",
      "3179250  Government Center - Cambridge St at Court St   2025-01-23   \n",
      "3179251  Government Center - Cambridge St at Court St   2025-01-02   \n",
      "3179252  Government Center - Cambridge St at Court St   2025-01-02   \n",
      "3179253  Government Center - Cambridge St at Court St   2025-01-24   \n",
      "\n",
      "               started_time  ended_date          ended_time  trip_duration  \\\n",
      "0       2025-04-07 12:16:49  2024-01-31 2025-04-07 12:21:02       4.216667   \n",
      "1       2025-04-07 08:14:16  2024-01-12 2025-04-07 08:19:48       5.533333   \n",
      "2       2025-04-07 15:00:05  2024-01-29 2025-04-07 15:05:47       5.700000   \n",
      "3       2025-04-07 16:33:40  2024-01-09 2025-04-07 17:00:41      27.016667   \n",
      "4       2025-04-07 10:19:21  2024-01-23 2025-04-07 10:31:39      12.300000   \n",
      "...                     ...         ...                 ...            ...   \n",
      "3179249 2025-04-07 09:01:14  2025-01-09 2025-04-07 09:06:27       5.217867   \n",
      "3179250 2025-04-07 17:06:57  2025-01-23 2025-04-07 17:08:51       1.907200   \n",
      "3179251 2025-04-07 10:31:00  2025-01-02 2025-04-07 10:32:53       1.882783   \n",
      "3179252 2025-04-07 10:36:28  2025-01-02 2025-04-07 10:38:10       1.701683   \n",
      "3179253 2025-04-07 19:37:12  2025-01-24 2025-04-07 19:47:20      10.139167   \n",
      "\n",
      "        startmonth endmonth  ... end_count total_activity  threshold  \\\n",
      "0          January  January  ...   31530.0        59961.0    20210.0   \n",
      "1          January  January  ...   31530.0        59961.0    20210.0   \n",
      "2          January  January  ...    5516.0        11231.0    20210.0   \n",
      "3          January  January  ...   31530.0        59961.0    20210.0   \n",
      "4          January  January  ...   10410.0        20676.0    20210.0   \n",
      "...            ...      ...  ...       ...            ...        ...   \n",
      "3179249    January  January  ...     820.0         1646.0    20210.0   \n",
      "3179250    January  January  ...   16977.0        33679.0    20210.0   \n",
      "3179251    January  January  ...   16977.0        33679.0    20210.0   \n",
      "3179252    January  January  ...   16977.0        33679.0    20210.0   \n",
      "3179253    January  January  ...   11364.0        22892.0    20210.0   \n",
      "\n",
      "         high_usage start_day_of_week_encoded end_day_of_week_encoded  \\\n",
      "0               1.0                         6                       6   \n",
      "1               1.0                         0                       0   \n",
      "2               0.0                         1                       1   \n",
      "3               1.0                         5                       5   \n",
      "4               1.0                         5                       5   \n",
      "...             ...                       ...                     ...   \n",
      "3179249         0.0                         4                       4   \n",
      "3179250         1.0                         4                       4   \n",
      "3179251         1.0                         4                       4   \n",
      "3179252         1.0                         4                       4   \n",
      "3179253         1.0                         0                       0   \n",
      "\n",
      "         endTOD_encoded  startTOD_encoded  endmonth_encoded  \\\n",
      "0                     0                 0                 3   \n",
      "1                     2                 2                 3   \n",
      "2                     0                 0                 3   \n",
      "3                     0                 0                 3   \n",
      "4                     2                 2                 3   \n",
      "...                 ...               ...               ...   \n",
      "3179249               2                 2                 3   \n",
      "3179250               0                 0                 3   \n",
      "3179251               2                 2                 3   \n",
      "3179252               2                 2                 3   \n",
      "3179253               1                 1                 3   \n",
      "\n",
      "         startmonth_encoded  \n",
      "0                         4  \n",
      "1                         4  \n",
      "2                         4  \n",
      "3                         4  \n",
      "4                         4  \n",
      "...                     ...  \n",
      "3179249                   4  \n",
      "3179250                   4  \n",
      "3179251                   4  \n",
      "3179252                   4  \n",
      "3179253                   4  \n",
      "\n",
      "[3178292 rows x 27 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(bikeData.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: [0.         0.74694898]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "trainX_scaled = scaler.fit_transform(X_train)  \n",
    "testX_scaled = scaler.transform(X_test) \n",
    "\n",
    "# Train the classifier\n",
    "clf = LogisticRegression(random_state=0).fit(trainX_scaled, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = clf.predict(testX_scaled)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
