{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS340-H Final Capstone Project\n",
    "Jennifer Ruffin\n",
    "\n",
    "**_Research Questions_**\n",
    "1. How does station-level demand (in terms of trip origins  and destinations) vary across different months and times of day?\n",
    "2. what are the resulting peak usage periods for the most popular stations?\n",
    "\n",
    "In this notebook, I will employ the machine learning methods Logistic Regression and Random Forests as well as the probabilisitc based regression methods Negative Binomial Regression and Poisson Regression to predict which stations are more likely to be used on a given day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bikeData = pd.read_csv('/Users/jenniferruffin/Desktop/finalcapstone.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                     int64\n",
      "start_station_name            object\n",
      "end_station_name              object\n",
      "started_date                  object\n",
      "started_time          datetime64[ns]\n",
      "ended_date                    object\n",
      "ended_time            datetime64[ns]\n",
      "trip_duration                float64\n",
      "startmonth                    object\n",
      "endmonth                      object\n",
      "startTOD                      object\n",
      "endTOD                        object\n",
      "start_hour                     int64\n",
      "end_hour                       int64\n",
      "start_day_of_week             object\n",
      "end_day_of_week               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "bikeData['started_time'] = pd.to_datetime(bikeData['started_time'], errors='coerce')\n",
    "bikeData['ended_time'] = pd.to_datetime(bikeData['ended_time'], errors='coerce')\n",
    "\n",
    "print(bikeData.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Probabilistc-Based Regression Models for Predicting Start Counts (Poisson and Negative Binomial...thanks MATH/STAT 220!)\n",
    "\n",
    "Step 1: Impport necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Aggregate data to station-hour level for start counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    start_station_name   month  hour day_of_week  start_count\n",
      "0   Broadway and Cabot   April    14      Monday            2\n",
      "1   Broadway and Cabot  August     1     Tuesday            1\n",
      "2   Broadway and Cabot  August     2      Friday            1\n",
      "3   Broadway and Cabot  August     2    Saturday            1\n",
      "4   Broadway and Cabot  August     2    Thursday            2\n",
      "5   Broadway and Cabot  August     2     Tuesday            1\n",
      "6   Broadway and Cabot  August     3   Wednesday            1\n",
      "7   Broadway and Cabot  August     5   Wednesday            1\n",
      "8   Broadway and Cabot  August     6      Friday            2\n",
      "9   Broadway and Cabot  August     6      Monday            1\n"
     ]
    }
   ],
   "source": [
    "hourly_starts = bikeData.groupby(['start_station_name', 'startmonth', 'start_hour', 'start_day_of_week']).size().reset_index(name='start_count')\n",
    "hourly_starts = hourly_starts.rename(columns={'start_hour': 'hour','startmonth': 'month', 'start_day_of_week': 'day_of_week'})\n",
    "\n",
    "print(hourly_starts.head(10)) # visual representation of new dataframe to being used for these models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Defining and Encoding predictors and target variable\n",
    "\n",
    "_Because I had categorical predictors, I decided to encode them so running the models would be easier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['start_station_name', 'month', 'day_of_week']\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    encoders[col] = LabelEncoder()\n",
    "    hourly_starts[col + '_encoded'] = encoders[col].fit_transform(hourly_starts[col])\n",
    "\n",
    "# Add hour as a predictor\n",
    "hourly_starts['hour_of_day'] = hourly_starts['hour']\n",
    "\n",
    "# defining predictors and target\n",
    "predictors = ['start_station_name_encoded', 'month_encoded', 'day_of_week_encoded',\n",
    "                   'hour_of_day'] # Add more predictors if available\n",
    "target = 'start_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Split data into testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_p, test_p = train_test_split(hourly_starts, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Run Regressions\n",
    "\n",
    "- Block 1: Poisson\n",
    "- Block 2: Negative Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            start_count   No. Observations:               309016\n",
      "Model:                            GLM   Df Residuals:                   309011\n",
      "Model Family:                 Poisson   Df Model:                            4\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -1.9427e+06\n",
      "Date:                Tue, 08 Apr 2025   Deviance:                   2.8351e+06\n",
      "Time:                        14:45:32   Pearson chi2:                 4.29e+06\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):             0.2489\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      1.7561      0.002    725.109      0.000       1.751       1.761\n",
      "start_station_name_encoded    -0.0003   4.07e-06    -85.197      0.000      -0.000      -0.000\n",
      "month_encoded                 -0.0020      0.000     -8.336      0.000      -0.002      -0.002\n",
      "day_of_week_encoded            0.0151      0.000     48.325      0.000       0.014       0.016\n",
      "hour_of_day                    0.0301      0.000    277.246      0.000       0.030       0.030\n",
      "==============================================================================================\n",
      "\n",
      "Poisson Regression Predictions (first 10):\n",
      "20014      7.960869\n",
      "207007     7.384380\n",
      "139804     6.550410\n",
      "165819    11.440452\n",
      "148516    10.377116\n",
      "325612     9.056091\n",
      "92771     10.506011\n",
      "335979     6.938229\n",
      "35771      7.642752\n",
      "195607     8.616884\n",
      "dtype: float64\n",
      "Poisson Regression Mean Absolute Error: 6.87462300174746\n"
     ]
    }
   ],
   "source": [
    "# Poisson\n",
    "formula = f\"{target} ~ \" + \" + \".join(predictors)\n",
    "poisson_model = smf.glm(formula=formula, data=train_p, family=sm.families.Poisson()).fit()\n",
    "\n",
    "print(poisson_model.summary())\n",
    "\n",
    "# Make predictions \n",
    "predictions_poisson = poisson_model.predict(test_p)\n",
    "print(\"\\nPoisson Regression Predictions (first 10):\")\n",
    "print(predictions_poisson.head(10))\n",
    "\n",
    "# Evaluate predictions, using mean absolute error here\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_poisson = mean_absolute_error(test_p[target], predictions_poisson.round()) # Round predictions to integers\n",
    "print(f\"Poisson Regression Mean Absolute Error: {mae_poisson}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/genmod/families/family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.\n",
      "  warnings.warn(\"Negative binomial dispersion parameter alpha not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negative Binomial Regression Summary:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            start_count   No. Observations:               309016\n",
      "Model:                            GLM   Df Residuals:                   309011\n",
      "Model Family:        NegativeBinomial   Df Model:                            4\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -9.7238e+05\n",
      "Date:                Tue, 08 Apr 2025   Deviance:                   3.0349e+05\n",
      "Time:                        14:46:26   Pearson chi2:                 4.52e+05\n",
      "No. Iterations:                     9   Pseudo R-squ. (CS):            0.03982\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      1.6172      0.007    226.162      0.000       1.603       1.631\n",
      "start_station_name_encoded    -0.0003   1.24e-05    -27.021      0.000      -0.000      -0.000\n",
      "month_encoded                 -0.0021      0.001     -2.977      0.003      -0.004      -0.001\n",
      "day_of_week_encoded            0.0162      0.001     17.061      0.000       0.014       0.018\n",
      "hour_of_day                    0.0398      0.000    123.688      0.000       0.039       0.040\n",
      "==============================================================================================\n",
      "Negative Binomial Regression Mean Absolute Error: 6.909287424762152\n"
     ]
    }
   ],
   "source": [
    "# Negative Binomial Regression\n",
    "negativebinomial_model = smf.glm(formula=formula, data=train_p,\n",
    "                                  family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "print(\"\\nNegative Binomial Regression Summary:\")\n",
    "print(negativebinomial_model.summary())\n",
    "\n",
    "predictions_nb = negativebinomial_model.predict(test_p)\n",
    "mae_nb = mean_absolute_error(test_p[target], predictions_nb.round())\n",
    "print(f\"Negative Binomial Regression Mean Absolute Error: {mae_nb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Predicting Peak Usage Times with ML Strategies Logistic Regression and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Aggregate hourly start counts for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    start_station_name  hour day_of_week   month  hourly_starts\n",
      "0   Broadway and Cabot     0      Friday     May              1\n",
      "1   Broadway and Cabot     0    Thursday    July              1\n",
      "2   Broadway and Cabot     0   Wednesday    July              1\n",
      "3   Broadway and Cabot     1      Sunday    June              1\n",
      "4   Broadway and Cabot     1     Tuesday  August              1\n",
      "5   Broadway and Cabot     1     Tuesday    June              1\n",
      "6   Broadway and Cabot     2      Friday  August              1\n",
      "7   Broadway and Cabot     2      Monday    July              1\n",
      "8   Broadway and Cabot     2    Saturday  August              1\n",
      "9   Broadway and Cabot     2    Thursday  August              2\n"
     ]
    }
   ],
   "source": [
    "hourly_station_usage = bikeData.groupby(['start_station_name', 'start_hour','start_day_of_week','startmonth']).size().reset_index(name='hourly_starts')\n",
    "hourly_station_usage = hourly_station_usage.rename(columns={'start_hour': 'hour','start_day_of_week': 'day_of_week', 'startmonth': 'month'})\n",
    "\n",
    "print(hourly_station_usage.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Defining a threshold for **\"high usage\"** \n",
    "\n",
    "_For this context, we will look at the top 10% of hourly counts for each station_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    start_station_name  hour day_of_week   month  hourly_starts  is_peak\n",
      "0   Broadway and Cabot     0      Friday     May              1        0\n",
      "1   Broadway and Cabot     0    Thursday    July              1        0\n",
      "2   Broadway and Cabot     0   Wednesday    July              1        0\n",
      "3   Broadway and Cabot     1      Sunday    June              1        0\n",
      "4   Broadway and Cabot     1     Tuesday  August              1        0\n",
      "5   Broadway and Cabot     1     Tuesday    June              1        0\n",
      "6   Broadway and Cabot     2      Friday  August              1        0\n",
      "7   Broadway and Cabot     2      Monday    July              1        0\n",
      "8   Broadway and Cabot     2    Saturday  August              1        0\n",
      "9   Broadway and Cabot     2    Thursday  August              2        1\n"
     ]
    }
   ],
   "source": [
    "def define_hourly_high_usage(df, percentile=0.9):\n",
    "    df['hourly_threshold'] = df.groupby('start_station_name')['hourly_starts'].transform(lambda x: x.quantile(percentile)) # CS 315 help, lambda\n",
    "    df['is_peak'] = (df['hourly_starts'] >= df['hourly_threshold']).astype(int)\n",
    "    df = df.drop(columns=['hourly_threshold'])\n",
    "    return df\n",
    "\n",
    "hourly_usage_with_peak = define_hourly_high_usage(hourly_station_usage.copy()) # copied to not modify original dataframe\n",
    "print(hourly_usage_with_peak.head(10)) # 0 indicates not a peak, 1 indicates peak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Encoding and Defining Predictors -- similar to strategy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_peak = ['start_station_name', 'month', 'day_of_week']\n",
    "encoders_peak = {}\n",
    "for col in categorical_cols_peak:\n",
    "    encoders_peak[col] = LabelEncoder()\n",
    "    hourly_usage_with_peak[col + '_encoded'] = encoders_peak[col].fit_transform(hourly_usage_with_peak[col])\n",
    "\n",
    "# Define predictors and target\n",
    "predictors_peak = ['start_station_name_encoded', 'month_encoded', 'day_of_week_encoded', 'hour']\n",
    "target_peak = 'is_peak'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Splitting data into testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_peak, test_peak = train_test_split(hourly_usage_with_peak, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Run Models\n",
    "\n",
    "- Block 1: Logistic Regression\n",
    "- Block 2: Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression for Peak Usage Prediction:\n",
      "Accuracy: 0.8845899941751343\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94     68339\n",
      "           1       0.00      0.00      0.00      8916\n",
      "\n",
      "    accuracy                           0.88     77255\n",
      "   macro avg       0.44      0.50      0.47     77255\n",
      "weighted avg       0.78      0.88      0.83     77255\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "model_peak = LogisticRegression(random_state=42)\n",
    "model_peak.fit(train_peak[predictors_peak], train_peak[target_peak])\n",
    "\n",
    "# Make predictions\n",
    "predictions_peak_lr = model_peak.predict(test_peak[predictors_peak])\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nLogistic Regression for Peak Usage Prediction:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_peak[target_peak], predictions_peak_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(test_peak[target_peak], predictions_peak_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest for Peak Usage Prediction:\n",
      "Accuracy: 0.8495501909261537\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92     68339\n",
      "           1       0.34      0.32      0.33      8916\n",
      "\n",
      "    accuracy                           0.85     77255\n",
      "   macro avg       0.62      0.62      0.62     77255\n",
      "weighted avg       0.85      0.85      0.85     77255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forests\n",
    "rf_peak = RandomForestClassifier(random_state=42)\n",
    "rf_peak.fit(train_peak[predictors_peak], train_peak[target_peak])\n",
    "\n",
    "# Make predictions\n",
    "predictions_peak_rf = rf_peak.predict(test_peak[predictors_peak])\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nRandom Forest for Peak Usage Prediction:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_peak[target_peak], predictions_peak_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(test_peak[target_peak], predictions_peak_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
