{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS340-H Final Capstone Project\n",
    "Jennifer Ruffin\n",
    "\n",
    "**_Research Questions_**\n",
    "1. How does station-level demand (in terms of trip origins  and destinations) vary across different months and times of day?\n",
    "2. What are the resulting peak usage periods for the most popular stations?\n",
    "\n",
    "In this notebook, I will employ the machine learning methods Logistic Regression and Random Forests as well as the probabilisitc based regression methods Negative Binomial Regression and Poisson Regression to predict which stations are more likely to be used on a given day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bikeData = pd.read_csv('/Users/jenniferruffin/Desktop/capstoneR.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                     int64\n",
      "start_station_name            object\n",
      "started_time          datetime64[ns]\n",
      "trip_duration                float64\n",
      "startTOD                      object\n",
      "start_hour                     int64\n",
      "start_day_of_week             object\n",
      "startmonth                    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "bikeData['started_time'] = pd.to_datetime(bikeData['started_time'], errors='coerce')\n",
    "print(bikeData.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Probabilistc-Based Regression Models for Predicting Start Counts (Poisson and Negative Binomial...thanks MATH/STAT 220!)\n",
    "\n",
    "Step 1: Impport necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Aggregate data to station-hour level for start counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      start_station_name   month  hour day_of_week  \\\n",
      "0  Central Square at Mass Ave / Essex St  August     0      Friday   \n",
      "1  Central Square at Mass Ave / Essex St  August     0      Monday   \n",
      "2  Central Square at Mass Ave / Essex St  August     0    Saturday   \n",
      "3  Central Square at Mass Ave / Essex St  August     0      Sunday   \n",
      "4  Central Square at Mass Ave / Essex St  August     0    Thursday   \n",
      "5  Central Square at Mass Ave / Essex St  August     0     Tuesday   \n",
      "6  Central Square at Mass Ave / Essex St  August     0   Wednesday   \n",
      "7  Central Square at Mass Ave / Essex St  August     1      Friday   \n",
      "8  Central Square at Mass Ave / Essex St  August     1      Monday   \n",
      "9  Central Square at Mass Ave / Essex St  August     1    Saturday   \n",
      "\n",
      "   start_count  \n",
      "0           11  \n",
      "1            6  \n",
      "2           17  \n",
      "3           22  \n",
      "4            8  \n",
      "5            6  \n",
      "6            2  \n",
      "7            4  \n",
      "8            4  \n",
      "9           13  \n"
     ]
    }
   ],
   "source": [
    "hourly_starts = bikeData.groupby(['start_station_name', 'startmonth', 'start_hour', 'start_day_of_week']).size().reset_index(name='start_count')\n",
    "hourly_starts = hourly_starts.rename(columns={'start_hour': 'hour','startmonth': 'month', 'start_day_of_week': 'day_of_week'})\n",
    "\n",
    "print(hourly_starts.head(10)) # visual representation of new dataframe to being used for these models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Defining and Encoding predictors and target variable\n",
    "\n",
    "_Because I had categorical predictors, I decided to encode them so running the models would be easier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['start_station_name', 'month', 'day_of_week']\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    encoders[col] = LabelEncoder()\n",
    "    hourly_starts[col + '_encoded'] = encoders[col].fit_transform(hourly_starts[col])\n",
    "\n",
    "# Add hour as a predictor\n",
    "hourly_starts['hour_of_day'] = hourly_starts['hour']\n",
    "\n",
    "# defining predictors and target\n",
    "predictors = ['start_station_name_encoded', 'month_encoded', 'day_of_week_encoded',\n",
    "                   'hour_of_day'] # Add more predictors if available\n",
    "target = 'start_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Split data into testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_p, test_p = train_test_split(hourly_starts, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Run Regressions\n",
    "\n",
    "- Block 1: Poisson\n",
    "- Block 2: Negative Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            start_count   No. Observations:                 2527\n",
      "Model:                            GLM   Df Residuals:                     2522\n",
      "Model Family:                 Poisson   Df Model:                            4\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -32954.\n",
      "Date:                Tue, 08 Apr 2025   Deviance:                       53235.\n",
      "Time:                        23:02:21   Pearson chi2:                 5.22e+04\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):             0.9992\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      2.9064      0.011    262.830      0.000       2.885       2.928\n",
      "start_station_name_encoded     0.0115      0.002      5.179      0.000       0.007       0.016\n",
      "month_encoded                 -0.0406      0.003    -14.432      0.000      -0.046      -0.035\n",
      "day_of_week_encoded           -0.0086      0.002     -5.503      0.000      -0.012      -0.006\n",
      "hour_of_day                    0.0638      0.000    128.488      0.000       0.063       0.065\n",
      "==============================================================================================\n",
      "\n",
      "Poisson Regression Predictions (first 10):\n",
      "1098    73.390620\n",
      "1244    62.024222\n",
      "1095    66.524771\n",
      "2119    39.095741\n",
      "2734    29.347983\n",
      "874     39.011445\n",
      "453     66.333191\n",
      "3122    51.219684\n",
      "1523    53.176416\n",
      "2128    40.961337\n",
      "dtype: float64\n",
      "Poisson Regression Mean Absolute Error: 22.606012658227847\n"
     ]
    }
   ],
   "source": [
    "# Poisson\n",
    "formula = f\"{target} ~ \" + \" + \".join(predictors)\n",
    "poisson_model = smf.glm(formula=formula, data=train_p, family=sm.families.Poisson()).fit()\n",
    "\n",
    "print(poisson_model.summary())\n",
    "\n",
    "# Make predictions \n",
    "predictions_poisson = poisson_model.predict(test_p)\n",
    "print(\"\\nPoisson Regression Predictions (first 10):\")\n",
    "print(predictions_poisson.head(10))\n",
    "\n",
    "# Evaluate predictions, using mean absolute error here\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_poisson = mean_absolute_error(test_p[target], predictions_poisson.round()) # Round predictions to integers\n",
    "print(f\"Poisson Regression Mean Absolute Error: {mae_poisson}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negative Binomial Regression Summary:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            start_count   No. Observations:                 2527\n",
      "Model:                            GLM   Df Residuals:                     2522\n",
      "Model Family:        NegativeBinomial   Df Model:                            4\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -11534.\n",
      "Date:                Tue, 08 Apr 2025   Deviance:                       1794.1\n",
      "Time:                        23:02:32   Pearson chi2:                 1.39e+03\n",
      "No. Iterations:                    13   Pseudo R-squ. (CS):             0.2589\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      2.3327      0.065     35.685      0.000       2.205       2.461\n",
      "start_station_name_encoded     0.0190      0.014      1.320      0.187      -0.009       0.047\n",
      "month_encoded                 -0.0456      0.018     -2.501      0.012      -0.081      -0.010\n",
      "day_of_week_encoded           -0.0067      0.010     -0.658      0.511      -0.027       0.013\n",
      "hour_of_day                    0.1053      0.003     35.045      0.000       0.099       0.111\n",
      "==============================================================================================\n",
      "Negative Binomial Regression Mean Absolute Error: 24.995253164556964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/genmod/families/family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.\n",
      "  warnings.warn(\"Negative binomial dispersion parameter alpha not \"\n"
     ]
    }
   ],
   "source": [
    "# Negative Binomial Regression\n",
    "negativebinomial_model = smf.glm(formula=formula, data=train_p,\n",
    "                                  family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "print(\"\\nNegative Binomial Regression Summary:\")\n",
    "print(negativebinomial_model.summary())\n",
    "\n",
    "predictions_nb = negativebinomial_model.predict(test_p)\n",
    "mae_nb = mean_absolute_error(test_p[target], predictions_nb.round())\n",
    "print(f\"Negative Binomial Regression Mean Absolute Error: {mae_nb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Predicting Peak Usage Times with ML Strategies Logistic Regression and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Aggregate hourly start counts for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      start_station_name  hour day_of_week   month  \\\n",
      "0  Central Square at Mass Ave / Essex St     0      Friday  August   \n",
      "1  Central Square at Mass Ave / Essex St     0      Friday    July   \n",
      "2  Central Square at Mass Ave / Essex St     0      Friday    June   \n",
      "3  Central Square at Mass Ave / Essex St     0      Friday     May   \n",
      "4  Central Square at Mass Ave / Essex St     0      Monday  August   \n",
      "5  Central Square at Mass Ave / Essex St     0      Monday    July   \n",
      "6  Central Square at Mass Ave / Essex St     0      Monday    June   \n",
      "7  Central Square at Mass Ave / Essex St     0      Monday     May   \n",
      "8  Central Square at Mass Ave / Essex St     0    Saturday  August   \n",
      "9  Central Square at Mass Ave / Essex St     0    Saturday    July   \n",
      "\n",
      "   hourly_starts  \n",
      "0             11  \n",
      "1             12  \n",
      "2              8  \n",
      "3             17  \n",
      "4              6  \n",
      "5              8  \n",
      "6              2  \n",
      "7              7  \n",
      "8             17  \n",
      "9             22  \n"
     ]
    }
   ],
   "source": [
    "hourly_station_usage = bikeData.groupby(['start_station_name', 'start_hour','start_day_of_week','startmonth']).size().reset_index(name='hourly_starts')\n",
    "hourly_station_usage = hourly_station_usage.rename(columns={'start_hour': 'hour','start_day_of_week': 'day_of_week', 'startmonth': 'month'})\n",
    "\n",
    "print(hourly_station_usage.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Defining a threshold for **\"high usage\"** \n",
    "\n",
    "_For this context, we will look at the top 10% of hourly counts for each station_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      start_station_name  hour day_of_week   month  \\\n",
      "0  Central Square at Mass Ave / Essex St     0      Friday  August   \n",
      "1  Central Square at Mass Ave / Essex St     0      Friday    July   \n",
      "2  Central Square at Mass Ave / Essex St     0      Friday    June   \n",
      "3  Central Square at Mass Ave / Essex St     0      Friday     May   \n",
      "4  Central Square at Mass Ave / Essex St     0      Monday  August   \n",
      "5  Central Square at Mass Ave / Essex St     0      Monday    July   \n",
      "6  Central Square at Mass Ave / Essex St     0      Monday    June   \n",
      "7  Central Square at Mass Ave / Essex St     0      Monday     May   \n",
      "8  Central Square at Mass Ave / Essex St     0    Saturday  August   \n",
      "9  Central Square at Mass Ave / Essex St     0    Saturday    July   \n",
      "\n",
      "   hourly_starts  is_peak  \n",
      "0             11        0  \n",
      "1             12        0  \n",
      "2              8        0  \n",
      "3             17        0  \n",
      "4              6        0  \n",
      "5              8        0  \n",
      "6              2        0  \n",
      "7              7        0  \n",
      "8             17        0  \n",
      "9             22        0  \n"
     ]
    }
   ],
   "source": [
    "def define_hourly_high_usage(df, percentile=0.9):\n",
    "    df['hourly_threshold'] = df.groupby('start_station_name')['hourly_starts'].transform(lambda x: x.quantile(percentile)) # CS 315 help, lambda\n",
    "    df['is_peak'] = (df['hourly_starts'] >= df['hourly_threshold']).astype(int)\n",
    "    df = df.drop(columns=['hourly_threshold'])\n",
    "    return df\n",
    "\n",
    "hourly_usage_with_peak = define_hourly_high_usage(hourly_station_usage.copy()) # copied to not modify original dataframe\n",
    "print(hourly_usage_with_peak.head(10)) # 0 indicates not a peak, 1 indicates peak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Encoding and Defining Predictors -- similar to strategy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_peak = ['start_station_name', 'month', 'day_of_week']\n",
    "encoders_peak = {}\n",
    "for col in categorical_cols_peak:\n",
    "    encoders_peak[col] = LabelEncoder()\n",
    "    hourly_usage_with_peak[col + '_encoded'] = encoders_peak[col].fit_transform(hourly_usage_with_peak[col])\n",
    "\n",
    "# Define predictors and target\n",
    "predictors_peak = ['start_station_name_encoded', 'month_encoded', 'day_of_week_encoded', 'hour']\n",
    "target_peak = 'is_peak'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Splitting data into testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_peak, test_peak = train_test_split(hourly_usage_with_peak, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Run Models\n",
    "\n",
    "- Block 1: Logistic Regression\n",
    "- Block 2: Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression for Peak Usage Prediction:\n",
      "Accuracy: 0.8876582278481012\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94       561\n",
      "           1       0.00      0.00      0.00        71\n",
      "\n",
      "    accuracy                           0.89       632\n",
      "   macro avg       0.44      0.50      0.47       632\n",
      "weighted avg       0.79      0.89      0.83       632\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "model_peak = LogisticRegression(random_state=42)\n",
    "model_peak.fit(train_peak[predictors_peak], train_peak[target_peak])\n",
    "\n",
    "# Make predictions\n",
    "predictions_peak_lr = model_peak.predict(test_peak[predictors_peak])\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nLogistic Regression for Peak Usage Prediction:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_peak[target_peak], predictions_peak_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(test_peak[target_peak], predictions_peak_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest for Peak Usage Prediction:\n",
      "Accuracy: 0.935126582278481\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       561\n",
      "           1       0.77      0.61      0.68        71\n",
      "\n",
      "    accuracy                           0.94       632\n",
      "   macro avg       0.86      0.79      0.82       632\n",
      "weighted avg       0.93      0.94      0.93       632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forests\n",
    "rf_peak = RandomForestClassifier(random_state=42)\n",
    "rf_peak.fit(train_peak[predictors_peak], train_peak[target_peak])\n",
    "\n",
    "# Make predictions\n",
    "predictions_peak_rf = rf_peak.predict(test_peak[predictors_peak])\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nRandom Forest for Peak Usage Prediction:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_peak[target_peak], predictions_peak_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(test_peak[target_peak], predictions_peak_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
